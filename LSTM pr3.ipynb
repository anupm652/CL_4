{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design RNN or its variant including LSTM or GRU a) Select a suitable time series dataset.\n",
    "b) Apply for predic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000 \n",
    "maxlen = 100 \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) # Pad sequences to have a consistent length for the input to the RNN\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen) \n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128)) \n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 148s 184ms/step - loss: 0.4178 - accuracy: 0.8056 - val_loss: 0.3654 - val_accuracy: 0.8424\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 156s 200ms/step - loss: 0.2610 - accuracy: 0.8958 - val_loss: 0.3608 - val_accuracy: 0.8492\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 154s 197ms/step - loss: 0.1935 - accuracy: 0.9271 - val_loss: 0.3963 - val_accuracy: 0.8359\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 183s 233ms/step - loss: 0.1407 - accuracy: 0.9493 - val_loss: 0.4517 - val_accuracy: 0.8348\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 189s 241ms/step - loss: 0.1135 - accuracy: 0.9593 - val_loss: 0.5227 - val_accuracy: 0.8334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24fe3be2fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=5, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 41s 52ms/step - loss: 0.5227 - accuracy: 0.8334\n",
      "Test Loss: 0.5226637125015259\n",
      "Test Accuracy: 0.8333600163459778\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " LSTM (Long Short-Term Memory) is a type of recurrent neural network (RNN) architecture designed to effectively capture and remember long-term dependencies in sequential data. It achieves this by using specialized memory cells and gating mechanisms that control the flow of information, allowing it to retain relevant information over long time intervals and mitigate the vanishing gradient problem commonly encountered in traditional RNNs. LSTM networks are widely used in natural language processing, time series analysis, speech recognition, and other tasks involving sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU (Gated Recurrent Unit) is a type of recurrent neural network (RNN) architecture designed for modeling sequential data. It's similar to LSTM but has a simpler structure with fewer parameters. GRU units contain gating mechanisms that regulate the flow of information within the network, allowing it to capture and retain relevant information over time while mitigating the vanishing gradient problem. GRU networks are commonly used in natural language processing, machine translation, and other tasks involving sequential data due to their efficiency and effectiveness in capturing long-range dependencies.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
